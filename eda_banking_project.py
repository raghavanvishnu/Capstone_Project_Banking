# -*- coding: utf-8 -*-
"""EDA_Banking_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kSCeTDELVf4vCF4xlAWDK09Jrz3eU_Ci

#WE LOAD THE DATASET HERE AND #CHECK IT FOR VARIOUS PARAMETERS## EDA AND THEN CLEANING
"""

# === Cell 1: Setup (run first) ===============================================
import os, gc, textwrap, numpy as np, pandas as pd

pd.set_option("display.max_columns", 100)
pd.set_option("display.float_format", lambda x: f"{x:,.4f}")
print("âœ… Libraries loaded.")

# === Cell 2 (Option A): Load from local upload ================================
# If your CSV is on your computer, run this cell, pick the file, and use that path below.
from google.colab import files
#uploaded = files.upload()
#csv_path = pd.read_csv("/content/Fraud Detection System for JPMorgan Chase _log.csv")#list(uploaded.keys())[0]

csv_path = "/content/Fraud Detection System for JPMorgan Chase _log.csv"
print("ðŸ“„ Using file:", csv_path)

# === Cell 2 (Option B): Load from Google Drive ===============================
# If your CSV is in Drive, run this instead of Cell 2(A), and set csv_path accordingly.
# from google.colab import drive
# drive.mount('/content/drive')
# csv_path = "/content/drive/MyDrive/path/to/your/Fraud Detection System for JPMorgan Chase _log.csv"
# print("ðŸ“„ Using file:", csv_path)

# === Cell 3: Memory-optimized read ===========================================
# Known columns in this dataset
usecols = [
    "step","type","amount","nameOrig","oldbalanceOrg","newbalanceOrig",
    "nameDest","oldbalanceDest","newbalanceDest","isFraud","isFlaggedFraud"
]

dtypes = {
    "step": "int32",
    "type": "category",
    "amount": "float32",
    "nameOrig": "category",
    "oldbalanceOrg": "float32",
    "newbalanceOrig": "float32",
    "nameDest": "category",
    "oldbalanceDest": "float32",
    "newbalanceDest": "float32",
    "isFraud": "int8",
    "isFlaggedFraud": "int8",
}

df = pd.read_csv(csv_path, usecols=usecols, dtype=dtypes)
print("âœ… Loaded shape:", df.shape)
print(df.dtypes)

# === Cell 4: Basic sanity checks =============================================
print("Missing values per column:\n", df.isna().sum())
print("\nTransaction types:\n", df["type"].value_counts())
fraud_rate = df["isFraud"].mean()
print(f"\nFraud rate: {fraud_rate:.4%}")

# Quick peek
df.head(3)

# === Cell 5: Create safe numeric helpers =====================================
EPS = 1e-6

# Origin/destination "consistency" deltas (useful for anomaly signals)
# origin_delta ~ oldbalanceOrg - amount - newbalanceOrig
# dest_delta   ~ newbalanceDest - oldbalanceDest - amount
df["origin_delta"] = (df["oldbalanceOrg"] - df["amount"] - df["newbalanceOrig"]).astype("float32")
df["dest_delta"]   = (df["newbalanceDest"] - df["oldbalanceDest"] - df["amount"]).astype("float32")

# Ratios / flags that often help
df["amt_to_oldOrg_ratio"] = (df["amount"] / (df["oldbalanceOrg"].abs() + EPS)).astype("float32")
df["is_zero_bal_sender"]  = (df["oldbalanceOrg"].abs() < EPS).astype("int8")
df["is_zero_bal_dest"]    = (df["oldbalanceDest"].abs() < EPS).astype("int8")

# Absolute error flags (tune thresholds later during modeling)
df["origin_mismatch_flag"] = (df["origin_delta"].abs() > 1.0).astype("int8")
df["dest_mismatch_flag"]   = (df["dest_delta"].abs() > 1.0).astype("int8")

print("âœ… Feature columns added.")
df.head(3)

# === Cell 6: Optional â€” derive a datetime from 'step' ========================
# 'step' is typically an hour index in this dataset. If you want a concrete timestamp, set a start date.
# This is optional and mainly for EDA convenience.
START_TS = np.datetime64("2024-01-01T00:00:00")
df["txn_time"] = START_TS + pd.to_timedelta(df["step"].astype(int), unit="h")
df["txn_time"] = df["txn_time"].astype("datetime64[ns]")
print("ðŸ•’ txn_time created from step (assumes step=hour index).")
df[["step","txn_time"]].head(3)

# === Cell 6: Optional â€” derive a datetime from 'step' ========================
# 'step' is typically an hour index in this dataset. If you want a concrete timestamp, set a start date.
# This is optional and mainly for EDA convenience.
START_TS = np.datetime64("2024-01-01T00:00:00")
df["txn_time"] = START_TS + pd.to_timedelta(df["step"].astype(int), unit="h")
df["txn_time"] = df["txn_time"].astype("datetime64[ns]")
print("ðŸ•’ txn_time created from step (assumes step=hour index).")
df[["step","txn_time"]].head(3)

# === Cell 7: Light integrity checks ==========================================
summary = {
    "rows": len(df),
    "cols": df.shape[1],
    "n_customers": df["nameOrig"].nunique(),
    "n_destinations": df["nameDest"].nunique(),
    "fraud_rate": float(df["isFraud"].mean()),
}
summary

# === Cell 8: Save processed dataset ==========================================
# Save a full processed Parquet (fast & compressed)
os.makedirs("processed", exist_ok=True)
full_out = "processed/fraud_processed.parquet"
df.to_parquet(full_out, index=False)
print("ðŸ’¾ Saved:", full_out)

# Also save a manageable sample for fast EDA/model prototyping (e.g., 200k rows)
np.random.seed(42)
sample_n = min(200_000, len(df))
df_sample = df.sample(sample_n)
sample_out = "processed/fraud_processed_sample_200k.parquet"
df_sample.to_parquet(sample_out, index=False)
print("ðŸ’¾ Saved sample:", sample_out)

# Free some memory if needed
del df_sample; gc.collect()

# === Cell 9: Quick report (so you can paste into your doc) ===================
report = f"""
Rows: {summary['rows']:,}
Columns: {summary['cols']}
Unique senders: {summary['n_customers']:,}
Unique destinations: {summary['n_destinations']:,}
Fraud rate: {summary['fraud_rate']:.4%}

New features added:
- origin_delta, dest_delta
- amt_to_oldOrg_ratio
- is_zero_bal_sender, is_zero_bal_dest
- origin_mismatch_flag, dest_mismatch_flag
- txn_time (derived from step, optional)
"""
print(textwrap.dedent(report))

"""Hereâ€™s a clear, submission-ready bullet point list with explanations for your **Data Ingestion & Preprocessing** step.
You can put this into your capstone document under *Methodology â†’ Step 1*.

---

## **Data Ingestion & Preprocessing â€” Steps & Explanations**

1. **Imported Required Libraries**

   * Used `pandas` for data handling, `numpy` for numerical operations, and system utilities for file management.
   * Set Pandas display options for better visibility during analysis.

2. **Loaded the Dataset into the Environment**

   * Uploaded the provided CSV file directly into Google Colab.
   * Defined `usecols` to load only relevant columns, and set **data types (`dtype`)** to memory-efficient formats like `int8`, `float32`, and `category`.
   * This reduces memory usage and speeds up processing for large datasets (\~6.3 million rows).

3. **Initial Data Inspection**

   * Checked dataset shape, column names, and data types.
   * Verified transaction types and calculated the overall fraud rate.
   * Counted missing values to confirm data completeness.

4. **Feature Creation for Fraud Analysis**

   * **`origin_delta`** = Difference between expected and actual sender balances after the transaction.
   * **`dest_delta`** = Difference between expected and actual receiver balances after the transaction.
   * **`amt_to_oldOrg_ratio`** = Ratio of transaction amount to senderâ€™s previous balance.
   * **`is_zero_bal_sender` & `is_zero_bal_dest`** = Flags indicating zero balance before transaction (potentially suspicious).
   * **`origin_mismatch_flag` & `dest_mismatch_flag`** = Flags for mismatches greater than a threshold in balance changes.
   * These features can help models identify anomalies linked to fraudulent activity.

5. **Optional Time Feature Engineering**

   * Converted `step` (hour index) into an actual timestamp (`txn_time`) for time-based EDA.
   * Assumed a start date (e.g., `2024-01-01`) and added hours accordingly.

6. **Data Integrity Checks**

   * Counted unique senders and destinations to understand dataset diversity.
   * Verified that fraud labels (`isFraud`) were correctly loaded.

7. **Data Saving for Future Steps**

   * Saved **full processed dataset** as a Parquet file for efficient loading in modeling stages.
   * Created and saved a **200k row sample** for quick prototyping and EDA.
"""

