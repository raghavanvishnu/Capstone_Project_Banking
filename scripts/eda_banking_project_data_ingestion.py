# -*- coding: utf-8 -*-
"""EDA_Banking_Project_Data_Ingestion

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kSCeTDELVf4vCF4xlAWDK09Jrz3eU_Ci

WE LOAD THE DATASET HERE AND CHECK IT FOR VARIOUS PARAMETERS ‚Üí EDA AND THEN CLEANING
"""

# === Imports ==================================================================

import os
import gc
import textwrap

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Optional: Google Colab support (only used if running in Colab)
try:
    from google.colab import files  # noqa: F401
except Exception:  # noqa: BLE001
    files = None


# === Cell 1: Setup (run first) ===============================================

pd.set_option("display.max_columns", 100)
pd.set_option("display.float_format", lambda x: f"{x:,.4f}")
print("‚úÖ Libraries loaded.")

# === Cell 2 (Option A): Load from local upload ================================
# If your CSV is on your computer, run this cell, pick the file, and use that path below.
# uploaded = files.upload()  # uncomment in Colab
# csv_path = list(uploaded.keys())[0]

# For Colab path or CI path, set directly:
csv_path = "/content/Fraud Detection System for JPMorgan Chase _log.csv"
print("üìÑ Using file:", csv_path)

# === Cell 2 (Option B): Load from Google Drive ===============================
# If your CSV is in Drive, run this instead of Cell 2(A), and set csv_path accordingly.
# from google.colab import drive
# drive.mount('/content/drive')
# csv_path = "/content/drive/MyDrive/path/to/your/Fraud Detection System for JPMorgan Chase _log.csv"
# print("üìÑ Using file:", csv_path)

# === Cell 3: Memory-optimized read ===========================================
# Known columns in this dataset
usecols = [
    "step",
    "type",
    "amount",
    "nameOrig",
    "oldbalanceOrg",
    "newbalanceOrig",
    "nameDest",
    "oldbalanceDest",
    "newbalanceDest",
    "isFraud",
    "isFlaggedFraud",
]

dtypes = {
    "step": "int32",
    "type": "category",
    "amount": "float32",
    "nameOrig": "category",
    "oldbalanceOrg": "float32",
    "newbalanceOrig": "float32",
    "nameDest": "category",
    "oldbalanceDest": "float32",
    "newbalanceDest": "float32",
    "isFraud": "int8",
    "isFlaggedFraud": "int8",
}

df = pd.read_csv(csv_path, usecols=usecols, dtype=dtypes)
print("‚úÖ Loaded shape:", df.shape)
print(df.dtypes)

# === Cell 4: Basic sanity checks =============================================
print("Missing values per column:\n", df.isna().sum())
print("\nTransaction types:\n", df["type"].value_counts())
fraud_rate = df["isFraud"].mean()
print(f"\nFraud rate: {fraud_rate:.4%}")

# Quick peek
_ = df.head(3)

# === Cell 5: Create safe numeric helpers =====================================
EPS = 1e-6

# Origin/destination "consistency" deltas (useful for anomaly signals)
# origin_delta ~ oldbalanceOrg - amount - newbalanceOrig
# dest_delta   ~ newbalanceDest - oldbalanceDest - amount
df["origin_delta"] = (df["oldbalanceOrg"] - df["amount"] - df["newbalanceOrig"]).astype(
    "float32"
)
df["dest_delta"] = (df["newbalanceDest"] - df["oldbalanceDest"] - df["amount"]).astype(
    "float32"
)

# Ratios / flags that often help
df["amt_to_oldOrg_ratio"] = (df["amount"] / (df["oldbalanceOrg"].abs() + EPS)).astype(
    "float32"
)
df["is_zero_bal_sender"] = (df["oldbalanceOrg"].abs() < EPS).astype("int8")
df["is_zero_bal_dest"] = (df["oldbalanceDest"].abs() < EPS).astype("int8")

# Absolute error flags (tune thresholds later during modeling)
df["origin_mismatch_flag"] = (df["origin_delta"].abs() > 1.0).astype("int8")
df["dest_mismatch_flag"] = (df["dest_delta"].abs() > 1.0).astype("int8")

print("‚úÖ Feature columns added.")
_ = df.head(3)

# === Cell 6: Optional ‚Äî derive a datetime from 'step' ========================
# 'step' is typically an hour index in this dataset. If you want a concrete timestamp, set a start date.
# This is optional and mainly for EDA convenience.
START_TS = np.datetime64("2024-01-01T00:00:00")
df["txn_time"] = START_TS + pd.to_timedelta(df["step"].astype(int), unit="h")
df["txn_time"] = df["txn_time"].astype("datetime64[ns]")
print("üïí txn_time created from step (assumes step=hour index).")
_ = df[["step", "txn_time"]].head(3)

# === Cell 7: Light integrity checks ==========================================
summary = {
    "rows": len(df),
    "cols": df.shape[1],
    "n_customers": df["nameOrig"].nunique(),
    "n_destinations": df["nameDest"].nunique(),
    "fraud_rate": float(df["isFraud"].mean()),
}
summary

# === Cell 8: Save processed dataset ==========================================
# Save a full processed Parquet (fast & compressed)
os.makedirs("processed", exist_ok=True)
full_out = "processed/fraud_processed.parquet"
df.to_parquet(full_out, index=False)
print("üíæ Saved:", full_out)

# Also save a manageable sample for fast EDA/model prototyping (e.g., 200k rows)
np.random.seed(42)
sample_n = min(200_000, len(df))
df_sample = df.sample(sample_n)
sample_out = "processed/fraud_processed_sample_200k.parquet"
df_sample.to_parquet(sample_out, index=False)
print("üíæ Saved sample:", sample_out)

# Free some memory if needed
del df_sample
gc.collect()

# === Cell 9: Quick report (so you can paste into your doc) ===================
report = f"""
Rows: {summary['rows']:,}
Columns: {summary['cols']}
Unique senders: {summary['n_customers']:,}
Unique destinations: {summary['n_destinations']:,}
Fraud rate: {summary['fraud_rate']:.4%}

New features added:
- origin_delta, dest_delta
- amt_to_oldOrg_ratio
- is_zero_bal_sender, is_zero_bal_dest
- origin_mismatch_flag, dest_mismatch_flag
- txn_time (derived from step, optional)
"""
print(textwrap.dedent(report))

# -----------------------------------------------------------------------------
# Data Ingestion & Preprocessing ‚Äî Steps & Explanations (for report)
# -----------------------------------------------------------------------------
"""
Put this into your capstone document under Methodology ‚Üí Step 1.

1) Imported Required Libraries
   - pandas for data handling, numpy for numerics, and system utilities.
   - Set Pandas display options for better visibility.

2) Loaded the Dataset into the Environment
   - Loaded selected columns with explicit dtypes (int8/float32/category) to reduce memory.
   - This speeds up processing for ~6.3M rows.

3) Initial Data Inspection
   - Verified shape, dtypes, missing values, and basic fraud rate.

4) Feature Creation for Fraud Analysis
   - origin_delta, dest_delta, amt_to_oldOrg_ratio, zero-balance flags,
     and mismatch flags; these often highlight anomalies linked to fraud.

5) Optional Time Feature Engineering
   - Derived txn_time from step (assumes step is an hour index).

6) Data Integrity Checks
   - Counted unique senders/destinations and confirmed labels loaded correctly.

7) Data Saving for Future Steps
   - Saved full processed Parquet and a 200k sample for fast iteration.
"""

# === EDA Step 1: Basic Dataset Overview ===
print("Shape:", df.shape)
print("\nData Types:\n", df.dtypes)
print("\nMissing Values:\n", df.isna().sum())
print("\nSummary Statistics:\n", df.describe())

# === EDA Step 2: Fraud Distribution ===
fraud_counts = df["isFraud"].value_counts()
fraud_percent = df["isFraud"].mean() * 100

plt.figure(figsize=(6, 4))
sns.barplot(x=fraud_counts.index, y=fraud_counts.values, palette="viridis")
plt.xticks([0, 1], ["Legit", "Fraud"])
plt.ylabel("Count")
plt.title(f"Fraud vs Legitimate Transactions\n(Fraud rate: {fraud_percent:.2f}%)")
plt.show()

# === EDA Step 3: Fraud by Transaction Type ===
fraud_by_type = df.groupby("type")["isFraud"].mean().sort_values(ascending=False) * 100
plt.figure(figsize=(8, 5))
sns.barplot(x=fraud_by_type.index, y=fraud_by_type.values, palette="coolwarm")
plt.ylabel("Fraud Rate (%)")
plt.title("Fraud Rate by Transaction Type")
plt.show()

print(fraud_by_type)

# === EDA Step 4: Amount Distribution ===
plt.figure(figsize=(8, 5))
sns.boxplot(x="isFraud", y="amount", data=df, showfliers=False, palette="mako")
plt.yscale("log")
plt.xticks([0, 1], ["Legit", "Fraud"])
plt.title("Transaction Amounts (Log Scale) - Fraud vs Legit")
plt.show()

# === EDA Step 5: Balance Behavior ===
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.boxplot(x="isFraud", y="origin_delta", data=df, showfliers=False, ax=axes[0])
axes[0].set_title("Origin Delta - Fraud vs Legit")

sns.boxplot(x="isFraud", y="dest_delta", data=df, showfliers=False, ax=axes[1])
axes[1].set_title("Destination Delta - Fraud vs Legit")
plt.show()

# === EDA Step 6: Time-based Fraud Analysis ===
fraud_time = df.groupby("step")["isFraud"].mean() * 100
plt.figure(figsize=(10, 4))
plt.plot(fraud_time.index, fraud_time.values, marker="o")
plt.xlabel("Step (Hour Index)")
plt.ylabel("Fraud Rate (%)")
plt.title("Fraud Rate over Time")
plt.show()

# === EDA Step 7: Correlation Heatmap (Numeric Only) ===
plt.figure(figsize=(10, 6))
corr = df.select_dtypes(include=["float32", "int8", "int32"]).corr()
sns.heatmap(corr, annot=False, cmap="coolwarm", center=0)
plt.title("Correlation Heatmap")
plt.show()

# === Save plots & tables helpers =============================================

PLOT_DIR = "plots/EDA"
TABLE_DIR = "reports/EDA_tables"
os.makedirs(PLOT_DIR, exist_ok=True)
os.makedirs(TABLE_DIR, exist_ok=True)


def save_fig(name: str, dpi: int = 200) -> None:
    """Save the current matplotlib figure into plots/EDA."""
    path = os.path.join(PLOT_DIR, f"{name}.png")
    plt.tight_layout()
    plt.savefig(path, dpi=dpi, bbox_inches="tight")
    print("üñºÔ∏è Saved:", path)


# Overview tables saved to CSV
overview = {
    "rows": [len(df)],
    "cols": [df.shape[1]],
    "fraud_rate": [df["isFraud"].mean()],
}
pd.DataFrame(overview).to_csv(os.path.join(TABLE_DIR, "overview.csv"), index=False)

(
    df.dtypes.astype(str)
    .reset_index()
    .rename(columns={"index": "column", 0: "dtype"})
    .to_csv(os.path.join(TABLE_DIR, "dtypes.csv"), index=False)
)

(
    df.isna()
    .sum()
    .reset_index()
    .rename(columns={"index": "column", 0: "missing"})
    .to_csv(os.path.join(TABLE_DIR, "missing_values.csv"), index=False)
)

df.describe().to_csv(os.path.join(TABLE_DIR, "describe_numeric.csv"))
print("üìÑ Saved: overview, dtypes, missing_values, describe_numeric")

# Save EDA plots
counts = df["isFraud"].value_counts().sort_index()
labels = ["Legit (0)", "Fraud (1)"]

plt.figure(figsize=(6, 4))
plt.bar(labels, counts.values)
plt.title(f"Fraud vs Legit (Fraud rate: {df['isFraud'].mean()*100:.2f}%)")
plt.ylabel("Count")
save_fig("01_fraud_distribution")
plt.show()

fraud_by_type = df.groupby("type")["isFraud"].mean().sort_values(ascending=False) * 100
fraud_by_type.to_csv(os.path.join(TABLE_DIR, "fraud_rate_by_type.csv"))

plt.figure(figsize=(8, 5))
plt.bar(fraud_by_type.index.astype(str), fraud_by_type.values)
plt.xticks(rotation=30, ha="right")
plt.ylabel("Fraud Rate (%)")
plt.title("Fraud Rate by Transaction Type")
save_fig("02_fraud_rate_by_type")
plt.show()

# Boxplot via matplotlib (no seaborn)
data_legit = df.loc[df["isFraud"] == 0, "amount"].clip(lower=0.01)
data_fraud = df.loc[df["isFraud"] == 1, "amount"].clip(lower=0.01)

plt.figure(figsize=(8, 5))
plt.boxplot(
    [np.log10(data_legit), np.log10(data_fraud)],
    labels=["Legit", "Fraud"],
    showfliers=False,
)
plt.ylabel("log10(Amount)")
plt.title("Transaction Amounts (Log Scale) ‚Äì Fraud vs Legit")
save_fig("03_amount_boxplot_log")
plt.show()

# Origin delta
plt.figure(figsize=(7, 4))
plt.boxplot(
    [
        df.loc[df["isFraud"] == 0, "origin_delta"],
        df.loc[df["isFraud"] == 1, "origin_delta"],
    ],
    labels=["Legit", "Fraud"],
    showfliers=False,
)
plt.title("Origin Delta ‚Äì Fraud vs Legit")
plt.ylabel("origin_delta")
save_fig("04_origin_delta_box")
plt.show()

# Destination delta
plt.figure(figsize=(7, 4))
plt.boxplot(
    [
        df.loc[df["isFraud"] == 0, "dest_delta"],
        df.loc[df["isFraud"] == 1, "dest_delta"],
    ],
    labels=["Legit", "Fraud"],
    showfliers=False,
)
plt.title("Destination Delta ‚Äì Fraud vs Legit")
plt.ylabel("dest_delta")
save_fig("05_dest_delta_box")
plt.show()

fraud_time = df.groupby("step")["isFraud"].mean() * 100

plt.figure(figsize=(10, 4))
plt.plot(fraud_time.index, fraud_time.values, marker="o", linewidth=1)
plt.xlabel("step (hour index)")
plt.ylabel("Fraud Rate (%)")
plt.title("Fraud Rate Over Time")
save_fig("06_fraud_rate_over_time")
plt.show()

fraud_time.to_csv(os.path.join(TABLE_DIR, "fraud_rate_over_step.csv"))

# Build a simple heatmap with matplotlib (no seaborn)
num_cols = df.select_dtypes(include=["int8", "int32", "float32"]).columns
corr_mat = df[num_cols].corr().values
labels = list(num_cols)

plt.figure(figsize=(9, 7))
plt.imshow(corr_mat, interpolation="nearest")
plt.colorbar()
plt.title("Correlation Heatmap (numeric)")
plt.xticks(range(len(labels)), labels, rotation=65, ha="right", fontsize=8)
plt.yticks(range(len(labels)), labels, fontsize=8)
save_fig("07_corr_heatmap_numeric")
plt.show()

# Findings scaffold (for report/PPT)
findings = f"""
EDA Key Notes (to paste into report/PPT)
---------------------------------------
Fraud rate: {df['isFraud'].mean()*100:.4f}%

Top risky transaction types (by fraud rate %):
{(df.groupby('type')['isFraud'].mean().sort_values(ascending=False)*100).round(3).head(5).to_string()}

Amount distribution: Fraud vs Legit shows higher median/upper tail for fraud (see 03_amount_boxplot_log.png).

Balance anomalies:
- origin_delta and dest_delta differ between fraud and legit (see 04_ and 05_ plots).

Time trend:
- See 06_fraud_rate_over_time.png for periods with elevated rates.

Next steps:
- Engineer velocity features and device/account change flags.
- Model with class-imbalance handling (class weights/SMOTE).
"""
out_path = os.path.join(TABLE_DIR, "eda_findings_scaffold.txt")
with open(out_path, "w", encoding="utf-8") as f:
    f.write(findings)
print("üìù Saved:", out_path)
print(findings)
